{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly Supervised Instance Segmentation using Class Peak Response \n",
    "### Evaluation code of object counting in COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from nest import modules, run_tasks\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "## make sure you have correctly install cocoapi, https://github.com/cocodataset/cocoapi\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import pycocotools.mask as mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "def mrmse(non_zero,count_pred, count_gt):\n",
    "    nzero_mask=torch.ones(count_gt.size())\n",
    "    if non_zero==1:\n",
    "        nzero_mask=torch.zeros(count_gt.size())\n",
    "        nzero_mask[count_gt!=0]=1\n",
    "    mrmse=torch.pow(count_pred - count_gt, 2)\n",
    "    mrmse = torch.mul(mrmse, nzero_mask)\n",
    "    mrmse = torch.sum(mrmse, 0)\n",
    "    nzero = torch.sum(nzero_mask, 0)\n",
    "    mrmse = torch.div(mrmse, nzero)\n",
    "    mrmse = torch.sqrt(mrmse)\n",
    "#     print(mrmse.size())\n",
    "    mrmse = torch.mean(mrmse)\n",
    "    return mrmse\n",
    "\n",
    "def rel_mrmse(non_zero,count_pred, count_gt):\n",
    "    nzero_mask=torch.ones(count_gt.size())\n",
    "    if non_zero==1:\n",
    "        nzero_mask=torch.zeros(count_gt.size())\n",
    "        nzero_mask[count_gt!=0]=1\n",
    "    num = torch.pow(count_pred - count_gt, 2)\n",
    "    denom = count_gt.clone()\n",
    "    denom = denom+1\n",
    "    rel_mrmse = torch.div(num, denom)\n",
    "    rel_mrmse = torch.mul(rel_mrmse, nzero_mask)\n",
    "    rel_mrmse = torch.sum(rel_mrmse, 0)\n",
    "    nzero = torch.sum(nzero_mask, 0)\n",
    "    rel_mrmse = torch.div(rel_mrmse, nzero)\n",
    "    rel_mrmse = torch.sqrt(rel_mrmse)\n",
    "    rel_mrmse = torch.mean(rel_mrmse)\n",
    "    return rel_mrmse\n",
    "\n",
    "# image pre-processor\n",
    "image_size = 448\n",
    "transformer = modules.image_transform(\n",
    "    image_size = [image_size, image_size],\n",
    "    augmentation = dict(),\n",
    "    mean = [0.485, 0.456, 0.406],\n",
    "    std = [0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.41s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "## load ground truth\n",
    "\n",
    "coco_path='/raid/xiaoke/MSCOCO/coco'\n",
    "cocoGt=COCO(coco_path+'/annotations/instances_val2014.json')\n",
    "image_ids=cocoGt.getImgIds()\n",
    "\n",
    "catids=cocoGt.getCatIds()\n",
    "\n",
    "num_classes=len(catids)\n",
    "catid2index={}\n",
    "for i,cid in enumerate(catids):\n",
    "    catid2index[cid]=i\n",
    "annids=cocoGt.getAnnIds()\n",
    "class_labels = OrderedDict()\n",
    "for id in annids:\n",
    "    anns=cocoGt.loadAnns(id)\n",
    "    for i in range(len(anns)):\n",
    "        ann=anns[i]\n",
    "        name=ann['image_id']\n",
    "        if name not in class_labels:\n",
    "            class_labels[name]=np.zeros(num_classes)\n",
    "        category_id=ann['category_id']\n",
    "        class_labels[name][catid2index[category_id]]+=1\n",
    "\n",
    "image_list_all=list(class_labels.keys())\n",
    "gt_counts_all=[gt[1] for gt in list(class_labels.items())]\n",
    "gt_counts_all=np.array(gt_counts_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function PeakResponseMapping._median_filter at 0x7ff46b559048>\n",
      "addedmodule5\n",
      "enable_peak_stimulation on\n"
     ]
    }
   ],
   "source": [
    "## use the first half as validation and second half as test\n",
    "index=int(40137/2)\n",
    "image_list=image_list_all[index:]\n",
    "gt_count=gt_counts_all[index:]\n",
    "\n",
    "## load model\n",
    "backbone = modules.fc_resnet50(channels=240, pretrained=False)\n",
    "model = modules.peak_response_mapping(backbone,enable_peak_stimulation=True,peak_stimulation='addedmodule5',\n",
    "                                     sub_pixel_locating_factor=1)\n",
    "model = nn.DataParallel(model)\n",
    "checkpoint = torch.load('../models/counting/coco14.pt')\n",
    "model_dict = model.state_dict()\n",
    "trained_dict = {k: v for k, v in checkpoint['model'].items() if k in model_dict}\n",
    "model_dict.update(trained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "model = model.module.cuda()\n",
    "#print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "tensor(1.9086)\n",
      "tensor(0.8489)\n",
      "tensor(0.3397)\n",
      "tensor(0.1788)\n"
     ]
    }
   ],
   "source": [
    "## counting\n",
    "pred_class=[]\n",
    "pred_count=[]\n",
    "for index_d,ima in enumerate(image_list):\n",
    "    if index_d%10000==0:\n",
    "        print(index_d)\n",
    "    raw_img = Image.open(coco_path+'/images/val2014/COCO_val2014_'+'0'*(12-len(str(ima)))+str(ima)+'.jpg').convert('RGB')\n",
    "    width, height=raw_img.size\n",
    "    input_var = transformer(raw_img).unsqueeze(0).cuda().requires_grad_()\n",
    "    model = model.eval()\n",
    "    confidence,class_response_map1,peak = model(input_var,1)\n",
    "    confidence=confidence.cpu().detach().numpy()\n",
    "    count_one = F.adaptive_avg_pool2d(class_response_map1, 1).squeeze(2).squeeze(2).detach().cpu().numpy()[0]\n",
    "    confidence[confidence<0]=0\n",
    "    confidence=confidence[0]\n",
    "    confidence[confidence>0]=1\n",
    "    pred_class.append(confidence)\n",
    "    pred_count.append(np.round(confidence*count_one))\n",
    "pred_count=np.array(pred_count)\n",
    "print(mrmse(1,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n",
    "print(rel_mrmse(1,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n",
    "print(mrmse(0,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n",
    "print(rel_mrmse(0,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
