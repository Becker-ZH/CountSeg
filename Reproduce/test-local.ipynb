{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly Supervised Instance Segmentation using Class Peak Response \n",
    "### Evaluation code of object counting in Pascal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nest import modules, run_tasks\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrmse(non_zero,count_pred, count_gt):\n",
    "    ## compute mrmse\n",
    "    nzero_mask=torch.ones(count_gt.size())\n",
    "    if non_zero==1:\n",
    "        nzero_mask=torch.zeros(count_gt.size())\n",
    "        nzero_mask[count_gt!=0]=1\n",
    "    mrmse=torch.pow(count_pred - count_gt, 2)\n",
    "    mrmse = torch.mul(mrmse, nzero_mask)\n",
    "    mrmse = torch.sum(mrmse, 0)\n",
    "    nzero = torch.sum(nzero_mask, 0)\n",
    "    mrmse = torch.div(mrmse, nzero)\n",
    "    mrmse = torch.sqrt(mrmse)\n",
    "    mrmse = torch.mean(mrmse)\n",
    "    return mrmse\n",
    "\n",
    "def rel_mrmse(non_zero,count_pred, count_gt):\n",
    "    ## compute reltive mrmse\n",
    "    nzero_mask=torch.ones(count_gt.size())\n",
    "    if non_zero==1:\n",
    "        nzero_mask=torch.zeros(count_gt.size())\n",
    "        nzero_mask[count_gt!=0]=1\n",
    "    num = torch.pow(count_pred - count_gt, 2)\n",
    "    denom = count_gt.clone()\n",
    "    denom = denom+1\n",
    "    rel_mrmse = torch.div(num, denom)\n",
    "    rel_mrmse = torch.mul(rel_mrmse, nzero_mask)\n",
    "    rel_mrmse = torch.sum(rel_mrmse, 0)\n",
    "    nzero = torch.sum(nzero_mask, 0)\n",
    "    rel_mrmse = torch.div(rel_mrmse, nzero)\n",
    "    rel_mrmse = torch.sqrt(rel_mrmse)\n",
    "    rel_mrmse = torch.mean(rel_mrmse)\n",
    "    return rel_mrmse\n",
    "\n",
    "def return_count_obj_rm_diff(xml_file,class_name):\n",
    "    count=0\n",
    "    tree = ET.parse(xml_file)\n",
    "    objs = tree.findall('object')\n",
    "    for ix, obj in enumerate(objs):\n",
    "        if obj.find('name').text==class_name and int(obj.find('difficult').text)==0:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\ncalling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\ncalling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\ncalling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\ncalling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\ncalling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
    }
   ],
   "source": [
    "# object categories\n",
    "class_names = modules.pascal_voc_object_categories()\n",
    "image_size = 448\n",
    "# image pre-processor\n",
    "transformer = modules.image_transform(\n",
    "    image_size = [image_size, image_size],\n",
    "    augmentation = dict(),\n",
    "    mean = [0.485, 0.456, 0.406],\n",
    "    std = [0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set pascal 2007 dataset path \n",
    "pascal_path='VOC2007'\n",
    "\n",
    "## load pascal 2007 test image list\n",
    "with open(pascal_path+'/ImageSets/Main/test.txt','r') as f:\n",
    "    image_list=[]\n",
    "    for ima in f:\n",
    "        ima=ima.strip('\\n')\n",
    "        image_list.append(ima)\n",
    "\n",
    "## get the ground truth count\n",
    "class_names = modules.pascal_voc_object_categories()\n",
    "num_classes=len(class_names)\n",
    "class_labels = OrderedDict()\n",
    "for class_idx in range(num_classes):\n",
    "    filename = os.path.join(\n",
    "        pascal_path+'/ImageSets/Main/', class_names[class_idx] + '_' + 'test' + '.txt')\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            name, label = line.split()\n",
    "            if name not in class_labels:\n",
    "                class_labels[name] = np.zeros(num_classes)\n",
    "            class_labels[name][class_idx] = int(label)\n",
    "            if int(label)!=-1:\n",
    "                count=return_count_obj_rm_diff(os.path.join(pascal_path+'/Annotations/',name+'.xml'),\n",
    "                    class_names[class_idx])\n",
    "                class_labels[name][class_idx] = int(count)\n",
    "            else:\n",
    "                class_labels[name][class_idx] = int(0)\n",
    "gt_count=[]\n",
    "for i in list(class_labels.items()):\n",
    "    gt_count.append(i[1])\n",
    "gt_count=np.array(gt_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<function PeakResponseMapping._median_filter at 0x7f7b07f1ac80>\naddedmodule5\nenable_peak_stimulation on\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\ntensor(1.1474)\ntensor(0.6134)\ntensor(0.2991)\ntensor(0.1836)\n"
    }
   ],
   "source": [
    "backbone = modules.fc_resnet50(channels=60, pretrained=False)\n",
    "model = modules.peak_response_mapping(backbone,enable_peak_stimulation=True,peak_stimulation='addedmodule5',\n",
    "                                     sub_pixel_locating_factor=1)\n",
    "\n",
    "f = open('all_ones.txt', 'w')\n",
    "\n",
    "# loaded pre-trained weights\n",
    "model = nn.DataParallel(model)\n",
    "state = torch.load('CountSeg/models/counting/pascal07.pt')\n",
    "model.load_state_dict(state['model'])\n",
    "model = model.module.cuda()\n",
    "model = model.eval()\n",
    "pred_count=[]\n",
    "for index_d,ima in enumerate(image_list):\n",
    "    if index_d%500==0:\n",
    "        print(index_d)\n",
    "    raw_img = Image.open(pascal_path+'/JPEGImages/'+str(ima)+'.jpg').convert('RGB')\n",
    "    width, height=raw_img.size\n",
    "    input_var = transformer(raw_img).unsqueeze(0).cuda().requires_grad_()\n",
    "    confidence,class_response_map1,peak = model(input_var,1)\n",
    "    confidence=confidence.cpu().detach().numpy()\n",
    "    count_one = F.adaptive_avg_pool2d(class_response_map1, 1).squeeze(2).squeeze(2).detach().cpu().numpy()[0]\n",
    "    confidence[confidence<0]=0\n",
    "    confidence=confidence[0]\n",
    "    confidence[confidence>0]=1\n",
    "    class_vector = np.round(confidence*count_one)\n",
    "    pred_count.append(class_vector)\n",
    "    cvaaa = np.array(class_vector)\n",
    "\n",
    "    #if not np.array_equal(np.array(gt_count[index_d]),cvaaa):\n",
    "    f.write('pic name:\\n')\n",
    "    f.write(str(ima)+'\\n')\n",
    "    f.write('standard:\\n')\n",
    "    f.write(str(gt_count[index_d]))\n",
    "    f.write('\\n')\n",
    "    f.write('prediction:\\n')\n",
    "    f.write(str(cvaaa))\n",
    "    f.write('\\n\\n\\n')\n",
    "\n",
    "    #! should have density maps\n",
    "\n",
    "f.close()\n",
    "\n",
    "pred_count=np.array(pred_count)\n",
    "print(mrmse(1,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n",
    "print(rel_mrmse(1,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n",
    "print(mrmse(0,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n",
    "print(rel_mrmse(0,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
