{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"ssap_exp_config":{"error_alert":"Error Occurs!","initial":[],"max_iteration":1,"recv_id":"","running":[],"summary":[],"version":"1.1.1"},"colab":{"name":"eval_counting_pascal07.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CNSyTQVE0ovy","colab_type":"text"},"source":["Change runtime type into GPU"]},{"cell_type":"code","metadata":{"id":"52KWRqK_0n03","colab_type":"code","colab":{}},"source":["!git clone https://github.com/timsufq/CountSeg.git\n","!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n","!bash Miniconda3-latest-Linux-x86_64.sh"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XSfja8Ge0z1K","colab_type":"code","colab":{}},"source":["!/root/miniconda3/bin/conda env create -f /content/CountSeg/environment.yml"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIlq8BhO0zha","colab_type":"code","colab":{}},"source":["!pip install torch==0.4.1\n","!pip install torchvision==0.2.2.post3\n","!pip install pillow==6.0.0\n","# Runnable Version with torch==0.4.1\n","# Restart Runtime can maintain the packages installed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KbPgaCdG0zXE","colab_type":"code","colab":{}},"source":["import torch\n","print(torch.__version__) # Check torch==0.4.1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YOLf5jfn054h","colab_type":"code","colab":{}},"source":["!pip install scipy==1.1.0 # this version has 'imresize'\n","!pip install visdom==0.1.8.8\n","!pip install git+https://github.com/szagoruyko/pyinn.git@master"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5k0Hoqgu05yr","colab_type":"code","colab":{}},"source":["!pip install git+https://github.com/ZhouYanzhao/Nest.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3HSDL4qF05r6","colab_type":"code","colab":{}},"source":["!nest module install /content/CountSeg/PRM-pytorch prm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5XGGxlg05ma","colab_type":"code","colab":{}},"source":["!nest module install /content/CountSeg/Nest-pytorch pytorch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfPaxKY91Cza","colab_type":"code","colab":{}},"source":["!ls # Check no '/content/Nest-pytorch'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XS6wsNU71Cuz","colab_type":"code","colab":{}},"source":["!nest module list --filter prm # and 'pytorch'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HDwZWLRd1Ybb","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DiGbjbcy1cmq","colab_type":"code","colab":{}},"source":["!unzip '/content/drive/My Drive/JHU-CV/VOCdevkit/VOC2007.zip'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nibwj0hm0fgz","colab_type":"text"},"source":["## Weakly Supervised Instance Segmentation using Class Peak Response \n","### Evaluation code of object counting in Pascal"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"o6J_wgKp0fg0","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","import json\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from nest import modules, run_tasks\n","import matplotlib.pyplot as plt\n","import random\n","import xml.etree.ElementTree as ET\n","from collections import OrderedDict\n","from PIL import Image\n","import cv2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_MdrA10f0fg8","colab_type":"code","colab":{}},"source":["def mrmse(non_zero,count_pred, count_gt):\n","    ## compute mrmse\n","    nzero_mask=torch.ones(count_gt.size())\n","    if non_zero==1:\n","        nzero_mask=torch.zeros(count_gt.size())\n","        nzero_mask[count_gt!=0]=1\n","    mrmse=torch.pow(count_pred - count_gt, 2)\n","    mrmse = torch.mul(mrmse, nzero_mask)\n","    mrmse = torch.sum(mrmse, 0)\n","    nzero = torch.sum(nzero_mask, 0)\n","    mrmse = torch.div(mrmse, nzero)\n","    mrmse = torch.sqrt(mrmse)\n","    mrmse = torch.mean(mrmse)\n","    return mrmse\n","\n","def rel_mrmse(non_zero,count_pred, count_gt):\n","    ## compute reltive mrmse\n","    nzero_mask=torch.ones(count_gt.size())\n","    if non_zero==1:\n","        nzero_mask=torch.zeros(count_gt.size())\n","        nzero_mask[count_gt!=0]=1\n","    num = torch.pow(count_pred - count_gt, 2)\n","    denom = count_gt.clone()\n","    denom = denom+1\n","    rel_mrmse = torch.div(num, denom)\n","    rel_mrmse = torch.mul(rel_mrmse, nzero_mask)\n","    rel_mrmse = torch.sum(rel_mrmse, 0)\n","    nzero = torch.sum(nzero_mask, 0)\n","    rel_mrmse = torch.div(rel_mrmse, nzero)\n","    rel_mrmse = torch.sqrt(rel_mrmse)\n","    rel_mrmse = torch.mean(rel_mrmse)\n","    return rel_mrmse\n","\n","def return_count_obj_rm_diff(xml_file,class_name):\n","    count=0\n","    tree = ET.parse(xml_file)\n","    objs = tree.findall('object')\n","    for ix, obj in enumerate(objs):\n","        if obj.find('name').text==class_name and int(obj.find('difficult').text)==0:\n","            count+=1\n","    return count"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"U4CwFyRE0fhF","colab_type":"code","colab":{}},"source":["# object categories\n","class_names = modules.pascal_voc_object_categories()\n","image_size = 448\n","# image pre-processor\n","transformer = modules.image_transform(\n","    image_size = [image_size, image_size],\n","    augmentation = dict(),\n","    mean = [0.485, 0.456, 0.406],\n","    std = [0.229, 0.224, 0.225])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXJ_iIYy0fhL","colab_type":"code","colab":{}},"source":["## set pascal 2007 dataset path \n","pascal_path='/content/VOC2007'\n","\n","## load pascal 2007 test image list\n","with open(pascal_path+'/ImageSets/Main/test.txt','r') as f:\n","    image_list=[]\n","    for ima in f:\n","        ima=ima.strip('\\n')\n","        image_list.append(ima)\n","\n","## get the ground truth count\n","class_names = modules.pascal_voc_object_categories()\n","num_classes=len(class_names)\n","class_labels = OrderedDict()\n","for class_idx in range(num_classes):\n","    filename = os.path.join(\n","        pascal_path+'/ImageSets/Main/', class_names[class_idx] + '_' + 'test' + '.txt')\n","    with open(filename, 'r') as f:\n","        for line in f:\n","            name, label = line.split()\n","            if name not in class_labels:\n","                class_labels[name] = np.zeros(num_classes)\n","            class_labels[name][class_idx] = int(label)\n","            if int(label)!=-1:\n","                count=return_count_obj_rm_diff(os.path.join(pascal_path+'/Annotations/',name+'.xml'),\n","                    class_names[class_idx])\n","                class_labels[name][class_idx] = int(count)\n","            else:\n","                class_labels[name][class_idx] = int(0)\n","gt_count=[]\n","for i in list(class_labels.items()):\n","    gt_count.append(i[1])\n","gt_count=np.array(gt_count)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jiAtVKcsV-Pb","colab_type":"code","outputId":"1964c55f-0ab8-48a3-9759-aa924cddfbea","executionInfo":{"status":"ok","timestamp":1587953566505,"user_tz":240,"elapsed":229628,"user":{"displayName":"Virtimmy Su","photoUrl":"","userId":"04452658251925823126"}},"colab":{"base_uri":"https://localhost:8080/","height":219}},"source":["!wget -c https://data.vision.ee.ethz.ch/kmaninis/share/COB/Precomputed/COB-Pascal-Main_trainvaltest_2007-proposals.tgz"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-04-27 02:08:59--  https://data.vision.ee.ethz.ch/kmaninis/share/COB/Precomputed/COB-Pascal-Main_trainvaltest_2007-proposals.tgz\n","Resolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.162\n","Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.162|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1948519133 (1.8G) [application/x-gzip]\n","Saving to: ‘COB-Pascal-Main_trainvaltest_2007-proposals.tgz’\n","\n","COB-Pascal-Main_tra 100%[===================>]   1.81G  7.77MB/s    in 3m 43s  \n","\n","2020-04-27 02:12:43 (8.35 MB/s) - ‘COB-Pascal-Main_trainvaltest_2007-proposals.tgz’ saved [1948519133/1948519133]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DFsCdCcWaMip","colab_type":"code","colab":{}},"source":["!cp /content/COB-Pascal-Main_trainvaltest_2007-proposals.tgz '/content/drive/My Drive'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekkgb5pVWNPK","colab_type":"code","colab":{}},"source":["!tar zxvf COB-Pascal-Main_trainvaltest_2007-proposals.tgz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMzGG26OW26Q","colab_type":"code","colab":{}},"source":["!ls COB-Pascal-Main_trainvaltest_2007-proposals"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-UfDZh2Hh4P7","colab_type":"code","colab":{}},"source":["!cp /content/COB-Pascal-Main_trainvaltest_2007-proposals/000175.mat '/content/drive/My Drive'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4OJ6Bj70fhR","colab_type":"code","colab":{}},"source":["backbone = modules.fc_resnet50(channels=60, pretrained=False)\n","model = modules.peak_response_mapping(backbone,enable_peak_stimulation=True,peak_stimulation='addedmodule5',\n","                                     sub_pixel_locating_factor=1)\n","# loaded pre-trained weights\n","model = nn.DataParallel(model)\n","state = torch.load('../models/counting/pascal07.pt')\n","model.load_state_dict(state['model'])\n","model = model.module.cuda()\n","model = model.eval()\n","pred_count=[]\n","for index_d,ima in enumerate(image_list):\n","    if index_d%500==0:\n","        print(index_d)\n","    raw_img = Image.open(pascal_path+'/JPEGImages/'+str(ima)+'.jpg').convert('RGB')\n","    width, height=raw_img.size\n","    input_var = transformer(raw_img).unsqueeze(0).cuda().requires_grad_()\n","    confidence,class_response_map1,peak = model(input_var,1)\n","    confidence=confidence.cpu().detach().numpy()\n","    count_one = F.adaptive_avg_pool2d(class_response_map1, 1).squeeze(2).squeeze(2).detach().cpu().numpy()[0]\n","    confidence[confidence<0]=0\n","    confidence=confidence[0]\n","    confidence[confidence>0]=1\n","    pred_count.append(np.round(confidence*count_one))\n","pred_count=np.array(pred_count)\n","print(mrmse(1,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n","print(rel_mrmse(1,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n","print(mrmse(0,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n","print(rel_mrmse(0,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))"],"execution_count":0,"outputs":[]}]}